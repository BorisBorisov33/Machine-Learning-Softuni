{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from skimage.io import imread\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Input,Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'notMNIST_small/'\n",
    "MAX_N_IMAGES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classes = os.listdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [os.path.join(BASE_DIR,image_class) for image_class in image_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "all_classes = []\n",
    "\n",
    "for image_class,directory in zip(image_classes,directories):\n",
    "    images = os.listdir(directory)[:MAX_N_IMAGES]\n",
    "    full_paths = [os.path.join(directory,image_path) for image_path in images]\n",
    "    all_images += [imread(img) for img in full_paths]\n",
    "    all_classes += [image_class]*MAX_N_IMAGES \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = np.array(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = np.array(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'A')"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(all_images[42])\n",
    "plt.title(all_classes[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images.min(),all_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_scaled = all_images/ 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,test_images,train_classes,test_classes = train_test_split(all_images_scaled,all_classes,test_size = 500,stratify = all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 28, 28)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 28, 28)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    200\n",
       "F    200\n",
       "E    200\n",
       "C    200\n",
       "A    200\n",
       "H    200\n",
       "J    200\n",
       "B    200\n",
       "G    200\n",
       "I    200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(all_classes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    150\n",
       "F    150\n",
       "E    150\n",
       "C    150\n",
       "A    150\n",
       "H    150\n",
       "J    150\n",
       "B    150\n",
       "G    150\n",
       "I    150\n",
       "dtype: int64"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_classes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    50\n",
       "F    50\n",
       "C    50\n",
       "B    50\n",
       "A    50\n",
       "H    50\n",
       "G    50\n",
       "E    50\n",
       "J    50\n",
       "I    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_classes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_vectors = train_images.reshape(len(train_images),-1)\n",
    "test_images_vectors = test_images.reshape(len(test_images),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images_vectors[30].reshape(28,28),cmap = \"gray\")\n",
    "plt.title(train_classes[30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = MLPClassifier(\n",
    "    hidden_layer_sizes=(10,),verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29291306\n",
      "Iteration 2, loss = 1.98248047\n",
      "Iteration 3, loss = 1.77816484\n",
      "Iteration 4, loss = 1.62073479\n",
      "Iteration 5, loss = 1.49263706\n",
      "Iteration 6, loss = 1.38998175\n",
      "Iteration 7, loss = 1.30556608\n",
      "Iteration 8, loss = 1.23249503\n",
      "Iteration 9, loss = 1.17517480\n",
      "Iteration 10, loss = 1.11928717\n",
      "Iteration 11, loss = 1.07154533\n",
      "Iteration 12, loss = 1.02917756\n",
      "Iteration 13, loss = 0.98608870\n",
      "Iteration 14, loss = 0.94634248\n",
      "Iteration 15, loss = 0.90917700\n",
      "Iteration 16, loss = 0.87344398\n",
      "Iteration 17, loss = 0.83898974\n",
      "Iteration 18, loss = 0.80888626\n",
      "Iteration 19, loss = 0.77472350\n",
      "Iteration 20, loss = 0.74370472\n",
      "Iteration 21, loss = 0.71621569\n",
      "Iteration 22, loss = 0.68952578\n",
      "Iteration 23, loss = 0.66109170\n",
      "Iteration 24, loss = 0.63685488\n",
      "Iteration 25, loss = 0.61355366\n",
      "Iteration 26, loss = 0.59381708\n",
      "Iteration 27, loss = 0.57577199\n",
      "Iteration 28, loss = 0.55784458\n",
      "Iteration 29, loss = 0.54326327\n",
      "Iteration 30, loss = 0.52815231\n",
      "Iteration 31, loss = 0.50882812\n",
      "Iteration 32, loss = 0.49426526\n",
      "Iteration 33, loss = 0.47897096\n",
      "Iteration 34, loss = 0.46758986\n",
      "Iteration 35, loss = 0.45603407\n",
      "Iteration 36, loss = 0.44376595\n",
      "Iteration 37, loss = 0.43707551\n",
      "Iteration 38, loss = 0.42811092\n",
      "Iteration 39, loss = 0.41980311\n",
      "Iteration 40, loss = 0.41266076\n",
      "Iteration 41, loss = 0.40673214\n",
      "Iteration 42, loss = 0.40276109\n",
      "Iteration 43, loss = 0.39478190\n",
      "Iteration 44, loss = 0.39038219\n",
      "Iteration 45, loss = 0.38266430\n",
      "Iteration 46, loss = 0.37909905\n",
      "Iteration 47, loss = 0.37484269\n",
      "Iteration 48, loss = 0.37079862\n",
      "Iteration 49, loss = 0.36568040\n",
      "Iteration 50, loss = 0.36093860\n",
      "Iteration 51, loss = 0.35821117\n",
      "Iteration 52, loss = 0.35414076\n",
      "Iteration 53, loss = 0.34970472\n",
      "Iteration 54, loss = 0.34539047\n",
      "Iteration 55, loss = 0.34158624\n",
      "Iteration 56, loss = 0.33964171\n",
      "Iteration 57, loss = 0.33479669\n",
      "Iteration 58, loss = 0.33184438\n",
      "Iteration 59, loss = 0.32741630\n",
      "Iteration 60, loss = 0.32546217\n",
      "Iteration 61, loss = 0.32198578\n",
      "Iteration 62, loss = 0.31826201\n",
      "Iteration 63, loss = 0.31655965\n",
      "Iteration 64, loss = 0.31291805\n",
      "Iteration 65, loss = 0.30932892\n",
      "Iteration 66, loss = 0.30593900\n",
      "Iteration 67, loss = 0.30442467\n",
      "Iteration 68, loss = 0.30074163\n",
      "Iteration 69, loss = 0.29989030\n",
      "Iteration 70, loss = 0.29531732\n",
      "Iteration 71, loss = 0.29362348\n",
      "Iteration 72, loss = 0.29153044\n",
      "Iteration 73, loss = 0.28958170\n",
      "Iteration 74, loss = 0.28656059\n",
      "Iteration 75, loss = 0.28507863\n",
      "Iteration 76, loss = 0.28287687\n",
      "Iteration 77, loss = 0.27833809\n",
      "Iteration 78, loss = 0.27563820\n",
      "Iteration 79, loss = 0.27516358\n",
      "Iteration 80, loss = 0.27451365\n",
      "Iteration 81, loss = 0.26866500\n",
      "Iteration 82, loss = 0.26671611\n",
      "Iteration 83, loss = 0.26447563\n",
      "Iteration 84, loss = 0.26292380\n",
      "Iteration 85, loss = 0.26140082\n",
      "Iteration 86, loss = 0.25854271\n",
      "Iteration 87, loss = 0.25690316\n",
      "Iteration 88, loss = 0.25581457\n",
      "Iteration 89, loss = 0.25343660\n",
      "Iteration 90, loss = 0.25086654\n",
      "Iteration 91, loss = 0.24702512\n",
      "Iteration 92, loss = 0.24679785\n",
      "Iteration 93, loss = 0.24761773\n",
      "Iteration 94, loss = 0.24279258\n",
      "Iteration 95, loss = 0.24001043\n",
      "Iteration 96, loss = 0.23868869\n",
      "Iteration 97, loss = 0.23631889\n",
      "Iteration 98, loss = 0.23496371\n",
      "Iteration 99, loss = 0.23248525\n",
      "Iteration 100, loss = 0.23170295\n",
      "Iteration 101, loss = 0.22997601\n",
      "Iteration 102, loss = 0.22745612\n",
      "Iteration 103, loss = 0.22752627\n",
      "Iteration 104, loss = 0.22439086\n",
      "Iteration 105, loss = 0.22130470\n",
      "Iteration 106, loss = 0.22070190\n",
      "Iteration 107, loss = 0.21795733\n",
      "Iteration 108, loss = 0.21579999\n",
      "Iteration 109, loss = 0.21533369\n",
      "Iteration 110, loss = 0.21375711\n",
      "Iteration 111, loss = 0.21274157\n",
      "Iteration 112, loss = 0.20959465\n",
      "Iteration 113, loss = 0.20874732\n",
      "Iteration 114, loss = 0.20661147\n",
      "Iteration 115, loss = 0.20709770\n",
      "Iteration 116, loss = 0.20743067\n",
      "Iteration 117, loss = 0.20716106\n",
      "Iteration 118, loss = 0.20456771\n",
      "Iteration 119, loss = 0.20026263\n",
      "Iteration 120, loss = 0.19838724\n",
      "Iteration 121, loss = 0.19689024\n",
      "Iteration 122, loss = 0.19442793\n",
      "Iteration 123, loss = 0.19334606\n",
      "Iteration 124, loss = 0.19274022\n",
      "Iteration 125, loss = 0.19163615\n",
      "Iteration 126, loss = 0.19020177\n",
      "Iteration 127, loss = 0.18698460\n",
      "Iteration 128, loss = 0.18656006\n",
      "Iteration 129, loss = 0.18398724\n",
      "Iteration 130, loss = 0.18309438\n",
      "Iteration 131, loss = 0.18171344\n",
      "Iteration 132, loss = 0.18063981\n",
      "Iteration 133, loss = 0.17884506\n",
      "Iteration 134, loss = 0.17815240\n",
      "Iteration 135, loss = 0.17744567\n",
      "Iteration 136, loss = 0.17610768\n",
      "Iteration 137, loss = 0.17494298\n",
      "Iteration 138, loss = 0.17188284\n",
      "Iteration 139, loss = 0.17259290\n",
      "Iteration 140, loss = 0.17018530\n",
      "Iteration 141, loss = 0.16906018\n",
      "Iteration 142, loss = 0.16852572\n",
      "Iteration 143, loss = 0.16839593\n",
      "Iteration 144, loss = 0.16612028\n",
      "Iteration 145, loss = 0.16494744\n",
      "Iteration 146, loss = 0.16296409\n",
      "Iteration 147, loss = 0.16110103\n",
      "Iteration 148, loss = 0.16188288\n",
      "Iteration 149, loss = 0.15828025\n",
      "Iteration 150, loss = 0.15767158\n",
      "Iteration 151, loss = 0.15634169\n",
      "Iteration 152, loss = 0.15523191\n",
      "Iteration 153, loss = 0.15474049\n",
      "Iteration 154, loss = 0.15313508\n",
      "Iteration 155, loss = 0.15134184\n",
      "Iteration 156, loss = 0.15116767\n",
      "Iteration 157, loss = 0.14942628\n",
      "Iteration 158, loss = 0.14754011\n",
      "Iteration 159, loss = 0.14763318\n",
      "Iteration 160, loss = 0.14620134\n",
      "Iteration 161, loss = 0.14528509\n",
      "Iteration 162, loss = 0.14445005\n",
      "Iteration 163, loss = 0.14232046\n",
      "Iteration 164, loss = 0.14212467\n",
      "Iteration 165, loss = 0.14253552\n",
      "Iteration 166, loss = 0.14090515\n",
      "Iteration 167, loss = 0.13891857\n",
      "Iteration 168, loss = 0.13862324\n",
      "Iteration 169, loss = 0.13728841\n",
      "Iteration 170, loss = 0.13698273\n",
      "Iteration 171, loss = 0.13532468\n",
      "Iteration 172, loss = 0.13497303\n",
      "Iteration 173, loss = 0.13307944\n",
      "Iteration 174, loss = 0.13116223\n",
      "Iteration 175, loss = 0.13119826\n",
      "Iteration 176, loss = 0.13130042\n",
      "Iteration 177, loss = 0.13270586\n",
      "Iteration 178, loss = 0.12966620\n",
      "Iteration 179, loss = 0.13059625\n",
      "Iteration 180, loss = 0.12658319\n",
      "Iteration 181, loss = 0.12631606\n",
      "Iteration 182, loss = 0.12549266\n",
      "Iteration 183, loss = 0.12355161\n",
      "Iteration 184, loss = 0.12283819\n",
      "Iteration 185, loss = 0.12219722\n",
      "Iteration 186, loss = 0.12176184\n",
      "Iteration 187, loss = 0.12233888\n",
      "Iteration 188, loss = 0.11983263\n",
      "Iteration 189, loss = 0.12037232\n",
      "Iteration 190, loss = 0.11722401\n",
      "Iteration 191, loss = 0.11807900\n",
      "Iteration 192, loss = 0.11589422\n",
      "Iteration 193, loss = 0.11488944\n",
      "Iteration 194, loss = 0.11507492\n",
      "Iteration 195, loss = 0.11437633\n",
      "Iteration 196, loss = 0.11384889\n",
      "Iteration 197, loss = 0.11289761\n",
      "Iteration 198, loss = 0.11174637\n",
      "Iteration 199, loss = 0.11174485\n",
      "Iteration 200, loss = 0.11019907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\as\\anaconda3\\envs\\baseREC\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10,), verbose=True)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.fit(train_images_vectors,train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813333333333333"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how good we overfitted \n",
    "neural_network.score(train_images_vectors,train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.score(test_images_vectors,test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more data increase the accuracy and decrease the bias! \\\n",
    "before increasing the number of the images we had high variance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.coefs_[1].shape\n",
    "# thesea are the coefs of the gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,5),\n",
    "    verbose = True,\n",
    "    max_iter = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.36428497\n",
      "Iteration 2, loss = 2.29731845\n",
      "Iteration 3, loss = 2.24314450\n",
      "Iteration 4, loss = 2.19122815\n",
      "Iteration 5, loss = 2.12360592\n",
      "Iteration 6, loss = 2.04074877\n",
      "Iteration 7, loss = 1.97137658\n",
      "Iteration 8, loss = 1.91020065\n",
      "Iteration 9, loss = 1.85595429\n",
      "Iteration 10, loss = 1.80385271\n",
      "Iteration 11, loss = 1.76383004\n",
      "Iteration 12, loss = 1.71713181\n",
      "Iteration 13, loss = 1.67761995\n",
      "Iteration 14, loss = 1.63852911\n",
      "Iteration 15, loss = 1.60485478\n",
      "Iteration 16, loss = 1.56780290\n",
      "Iteration 17, loss = 1.53158209\n",
      "Iteration 18, loss = 1.49493237\n",
      "Iteration 19, loss = 1.45476715\n",
      "Iteration 20, loss = 1.41575843\n",
      "Iteration 21, loss = 1.38235839\n",
      "Iteration 22, loss = 1.35577316\n",
      "Iteration 23, loss = 1.32163382\n",
      "Iteration 24, loss = 1.29382606\n",
      "Iteration 25, loss = 1.26549996\n",
      "Iteration 26, loss = 1.23888381\n",
      "Iteration 27, loss = 1.21397159\n",
      "Iteration 28, loss = 1.18902205\n",
      "Iteration 29, loss = 1.16309891\n",
      "Iteration 30, loss = 1.14286386\n",
      "Iteration 31, loss = 1.11999615\n",
      "Iteration 32, loss = 1.09665749\n",
      "Iteration 33, loss = 1.07815006\n",
      "Iteration 34, loss = 1.05813581\n",
      "Iteration 35, loss = 1.03769283\n",
      "Iteration 36, loss = 1.01532267\n",
      "Iteration 37, loss = 0.99514220\n",
      "Iteration 38, loss = 0.97742667\n",
      "Iteration 39, loss = 0.95802944\n",
      "Iteration 40, loss = 0.93841500\n",
      "Iteration 41, loss = 0.92303310\n",
      "Iteration 42, loss = 0.90194017\n",
      "Iteration 43, loss = 0.88445546\n",
      "Iteration 44, loss = 0.86285486\n",
      "Iteration 45, loss = 0.84351173\n",
      "Iteration 46, loss = 0.82334692\n",
      "Iteration 47, loss = 0.80504266\n",
      "Iteration 48, loss = 0.78913889\n",
      "Iteration 49, loss = 0.76880171\n",
      "Iteration 50, loss = 0.75041776\n",
      "Iteration 51, loss = 0.73668538\n",
      "Iteration 52, loss = 0.72346468\n",
      "Iteration 53, loss = 0.71453953\n",
      "Iteration 54, loss = 0.69874135\n",
      "Iteration 55, loss = 0.68098052\n",
      "Iteration 56, loss = 0.67422285\n",
      "Iteration 57, loss = 0.66112236\n",
      "Iteration 58, loss = 0.64972435\n",
      "Iteration 59, loss = 0.63818277\n",
      "Iteration 60, loss = 0.63189145\n",
      "Iteration 61, loss = 0.62154087\n",
      "Iteration 62, loss = 0.61127014\n",
      "Iteration 63, loss = 0.60127147\n",
      "Iteration 64, loss = 0.59472559\n",
      "Iteration 65, loss = 0.58650208\n",
      "Iteration 66, loss = 0.57990110\n",
      "Iteration 67, loss = 0.57375801\n",
      "Iteration 68, loss = 0.57012973\n",
      "Iteration 69, loss = 0.55961158\n",
      "Iteration 70, loss = 0.55453679\n",
      "Iteration 71, loss = 0.55626721\n",
      "Iteration 72, loss = 0.54912347\n",
      "Iteration 73, loss = 0.54498510\n",
      "Iteration 74, loss = 0.53218904\n",
      "Iteration 75, loss = 0.52936649\n",
      "Iteration 76, loss = 0.52523047\n",
      "Iteration 77, loss = 0.52421293\n",
      "Iteration 78, loss = 0.52171237\n",
      "Iteration 79, loss = 0.51301461\n",
      "Iteration 80, loss = 0.50587245\n",
      "Iteration 81, loss = 0.50351581\n",
      "Iteration 82, loss = 0.50193514\n",
      "Iteration 83, loss = 0.49867599\n",
      "Iteration 84, loss = 0.49425869\n",
      "Iteration 85, loss = 0.49120144\n",
      "Iteration 86, loss = 0.48432637\n",
      "Iteration 87, loss = 0.48295598\n",
      "Iteration 88, loss = 0.47761789\n",
      "Iteration 89, loss = 0.47264511\n",
      "Iteration 90, loss = 0.47037655\n",
      "Iteration 91, loss = 0.46829777\n",
      "Iteration 92, loss = 0.46464830\n",
      "Iteration 93, loss = 0.45968857\n",
      "Iteration 94, loss = 0.45795940\n",
      "Iteration 95, loss = 0.45538694\n",
      "Iteration 96, loss = 0.45215720\n",
      "Iteration 97, loss = 0.44895850\n",
      "Iteration 98, loss = 0.44635009\n",
      "Iteration 99, loss = 0.44257793\n",
      "Iteration 100, loss = 0.44132443\n",
      "Iteration 101, loss = 0.43993746\n",
      "Iteration 102, loss = 0.43640133\n",
      "Iteration 103, loss = 0.43473204\n",
      "Iteration 104, loss = 0.43927608\n",
      "Iteration 105, loss = 0.42956563\n",
      "Iteration 106, loss = 0.43526770\n",
      "Iteration 107, loss = 0.42756135\n",
      "Iteration 108, loss = 0.42536213\n",
      "Iteration 109, loss = 0.42760964\n",
      "Iteration 110, loss = 0.42357923\n",
      "Iteration 111, loss = 0.41863352\n",
      "Iteration 112, loss = 0.41234243\n",
      "Iteration 113, loss = 0.40929706\n",
      "Iteration 114, loss = 0.40857444\n",
      "Iteration 115, loss = 0.40513651\n",
      "Iteration 116, loss = 0.40978130\n",
      "Iteration 117, loss = 0.40593832\n",
      "Iteration 118, loss = 0.39770289\n",
      "Iteration 119, loss = 0.39504762\n",
      "Iteration 120, loss = 0.39273506\n",
      "Iteration 121, loss = 0.39153195\n",
      "Iteration 122, loss = 0.39089908\n",
      "Iteration 123, loss = 0.38590884\n",
      "Iteration 124, loss = 0.38600960\n",
      "Iteration 125, loss = 0.38358200\n",
      "Iteration 126, loss = 0.38593450\n",
      "Iteration 127, loss = 0.39472691\n",
      "Iteration 128, loss = 0.37993784\n",
      "Iteration 129, loss = 0.37914861\n",
      "Iteration 130, loss = 0.37944109\n",
      "Iteration 131, loss = 0.37273272\n",
      "Iteration 132, loss = 0.37307816\n",
      "Iteration 133, loss = 0.37103368\n",
      "Iteration 134, loss = 0.37018711\n",
      "Iteration 135, loss = 0.36522422\n",
      "Iteration 136, loss = 0.36242724\n",
      "Iteration 137, loss = 0.36466199\n",
      "Iteration 138, loss = 0.36024193\n",
      "Iteration 139, loss = 0.35877207\n",
      "Iteration 140, loss = 0.35763408\n",
      "Iteration 141, loss = 0.35640565\n",
      "Iteration 142, loss = 0.35168239\n",
      "Iteration 143, loss = 0.35100415\n",
      "Iteration 144, loss = 0.35039307\n",
      "Iteration 145, loss = 0.34936742\n",
      "Iteration 146, loss = 0.35681759\n",
      "Iteration 147, loss = 0.34587965\n",
      "Iteration 148, loss = 0.34542396\n",
      "Iteration 149, loss = 0.34547037\n",
      "Iteration 150, loss = 0.34320647\n",
      "Iteration 151, loss = 0.34120808\n",
      "Iteration 152, loss = 0.33876147\n",
      "Iteration 153, loss = 0.33543896\n",
      "Iteration 154, loss = 0.33461113\n",
      "Iteration 155, loss = 0.33352818\n",
      "Iteration 156, loss = 0.33012627\n",
      "Iteration 157, loss = 0.32976090\n",
      "Iteration 158, loss = 0.33031759\n",
      "Iteration 159, loss = 0.32828184\n",
      "Iteration 160, loss = 0.32808164\n",
      "Iteration 161, loss = 0.32417388\n",
      "Iteration 162, loss = 0.32324166\n",
      "Iteration 163, loss = 0.32118144\n",
      "Iteration 164, loss = 0.32291635\n",
      "Iteration 165, loss = 0.32339534\n",
      "Iteration 166, loss = 0.31920144\n",
      "Iteration 167, loss = 0.32829470\n",
      "Iteration 168, loss = 0.32302014\n",
      "Iteration 169, loss = 0.31715481\n",
      "Iteration 170, loss = 0.31632634\n",
      "Iteration 171, loss = 0.31603657\n",
      "Iteration 172, loss = 0.31006246\n",
      "Iteration 173, loss = 0.30708686\n",
      "Iteration 174, loss = 0.30507538\n",
      "Iteration 175, loss = 0.30655334\n",
      "Iteration 176, loss = 0.30185795\n",
      "Iteration 177, loss = 0.30478063\n",
      "Iteration 178, loss = 0.30164812\n",
      "Iteration 179, loss = 0.29951244\n",
      "Iteration 180, loss = 0.30029688\n",
      "Iteration 181, loss = 0.29904525\n",
      "Iteration 182, loss = 0.29510838\n",
      "Iteration 183, loss = 0.29435443\n",
      "Iteration 184, loss = 0.29361753\n",
      "Iteration 185, loss = 0.29420295\n",
      "Iteration 186, loss = 0.29156116\n",
      "Iteration 187, loss = 0.29295387\n",
      "Iteration 188, loss = 0.28727166\n",
      "Iteration 189, loss = 0.28890912\n",
      "Iteration 190, loss = 0.29028536\n",
      "Iteration 191, loss = 0.28813637\n",
      "Iteration 192, loss = 0.28643519\n",
      "Iteration 193, loss = 0.28370377\n",
      "Iteration 194, loss = 0.28179143\n",
      "Iteration 195, loss = 0.28058460\n",
      "Iteration 196, loss = 0.28150177\n",
      "Iteration 197, loss = 0.27898109\n",
      "Iteration 198, loss = 0.27746097\n",
      "Iteration 199, loss = 0.27449043\n",
      "Iteration 200, loss = 0.27507541\n",
      "Iteration 201, loss = 0.27284970\n",
      "Iteration 202, loss = 0.27618391\n",
      "Iteration 203, loss = 0.27051572\n",
      "Iteration 204, loss = 0.27278729\n",
      "Iteration 205, loss = 0.27285466\n",
      "Iteration 206, loss = 0.26916150\n",
      "Iteration 207, loss = 0.26628764\n",
      "Iteration 208, loss = 0.26840350\n",
      "Iteration 209, loss = 0.26570327\n",
      "Iteration 210, loss = 0.26610279\n",
      "Iteration 211, loss = 0.26194304\n",
      "Iteration 212, loss = 0.26137295\n",
      "Iteration 213, loss = 0.26452039\n",
      "Iteration 214, loss = 0.26592292\n",
      "Iteration 215, loss = 0.26168227\n",
      "Iteration 216, loss = 0.26253722\n",
      "Iteration 217, loss = 0.26203570\n",
      "Iteration 218, loss = 0.25611820\n",
      "Iteration 219, loss = 0.25633112\n",
      "Iteration 220, loss = 0.25375325\n",
      "Iteration 221, loss = 0.25654028\n",
      "Iteration 222, loss = 0.25362480\n",
      "Iteration 223, loss = 0.25247990\n",
      "Iteration 224, loss = 0.25238279\n",
      "Iteration 225, loss = 0.25151382\n",
      "Iteration 226, loss = 0.24777139\n",
      "Iteration 227, loss = 0.24905924\n",
      "Iteration 228, loss = 0.24667523\n",
      "Iteration 229, loss = 0.24537671\n",
      "Iteration 230, loss = 0.24397446\n",
      "Iteration 231, loss = 0.24470917\n",
      "Iteration 232, loss = 0.24542510\n",
      "Iteration 233, loss = 0.24453788\n",
      "Iteration 234, loss = 0.24016613\n",
      "Iteration 235, loss = 0.23832539\n",
      "Iteration 236, loss = 0.23880298\n",
      "Iteration 237, loss = 0.23695884\n",
      "Iteration 238, loss = 0.23525434\n",
      "Iteration 239, loss = 0.23517892\n",
      "Iteration 240, loss = 0.23664296\n",
      "Iteration 241, loss = 0.23476060\n",
      "Iteration 242, loss = 0.23821136\n",
      "Iteration 243, loss = 0.23117151\n",
      "Iteration 244, loss = 0.23229158\n",
      "Iteration 245, loss = 0.23762241\n",
      "Iteration 246, loss = 0.23128583\n",
      "Iteration 247, loss = 0.22996690\n",
      "Iteration 248, loss = 0.23090755\n",
      "Iteration 249, loss = 0.22918631\n",
      "Iteration 250, loss = 0.22635440\n",
      "Iteration 251, loss = 0.22506038\n",
      "Iteration 252, loss = 0.22663912\n",
      "Iteration 253, loss = 0.22309543\n",
      "Iteration 254, loss = 0.22095748\n",
      "Iteration 255, loss = 0.22176076\n",
      "Iteration 256, loss = 0.22025468\n",
      "Iteration 257, loss = 0.21917049\n",
      "Iteration 258, loss = 0.21905379\n",
      "Iteration 259, loss = 0.21813484\n",
      "Iteration 260, loss = 0.21827491\n",
      "Iteration 261, loss = 0.21621035\n",
      "Iteration 262, loss = 0.21601092\n",
      "Iteration 263, loss = 0.21643731\n",
      "Iteration 264, loss = 0.21853304\n",
      "Iteration 265, loss = 0.21281721\n",
      "Iteration 266, loss = 0.21307668\n",
      "Iteration 267, loss = 0.21159203\n",
      "Iteration 268, loss = 0.21194856\n",
      "Iteration 269, loss = 0.20944295\n",
      "Iteration 270, loss = 0.21147866\n",
      "Iteration 271, loss = 0.20859527\n",
      "Iteration 272, loss = 0.20793809\n",
      "Iteration 273, loss = 0.20649127\n",
      "Iteration 274, loss = 0.20804197\n",
      "Iteration 275, loss = 0.20672809\n",
      "Iteration 276, loss = 0.20503528\n",
      "Iteration 277, loss = 0.20366806\n",
      "Iteration 278, loss = 0.20332851\n",
      "Iteration 279, loss = 0.20214423\n",
      "Iteration 280, loss = 0.20240170\n",
      "Iteration 281, loss = 0.20093806\n",
      "Iteration 282, loss = 0.20124187\n",
      "Iteration 283, loss = 0.19983818\n",
      "Iteration 284, loss = 0.19868463\n",
      "Iteration 285, loss = 0.19768894\n",
      "Iteration 286, loss = 0.19583543\n",
      "Iteration 287, loss = 0.19602141\n",
      "Iteration 288, loss = 0.19730839\n",
      "Iteration 289, loss = 0.19614698\n",
      "Iteration 290, loss = 0.19468975\n",
      "Iteration 291, loss = 0.19676816\n",
      "Iteration 292, loss = 0.19378484\n",
      "Iteration 293, loss = 0.19393100\n",
      "Iteration 294, loss = 0.19124782\n",
      "Iteration 295, loss = 0.19064383\n",
      "Iteration 296, loss = 0.18969642\n",
      "Iteration 297, loss = 0.19049537\n",
      "Iteration 298, loss = 0.18876931\n",
      "Iteration 299, loss = 0.18932407\n",
      "Iteration 300, loss = 0.19159739\n",
      "Iteration 301, loss = 0.18816941\n",
      "Iteration 302, loss = 0.18831971\n",
      "Iteration 303, loss = 0.18501634\n",
      "Iteration 304, loss = 0.18619939\n",
      "Iteration 305, loss = 0.18372209\n",
      "Iteration 306, loss = 0.18306905\n",
      "Iteration 307, loss = 0.18058045\n",
      "Iteration 308, loss = 0.18283769\n",
      "Iteration 309, loss = 0.18045840\n",
      "Iteration 310, loss = 0.17954864\n",
      "Iteration 311, loss = 0.18023720\n",
      "Iteration 312, loss = 0.18368663\n",
      "Iteration 313, loss = 0.17877997\n",
      "Iteration 314, loss = 0.17539390\n",
      "Iteration 315, loss = 0.17724179\n",
      "Iteration 316, loss = 0.17393625\n",
      "Iteration 317, loss = 0.17813908\n",
      "Iteration 318, loss = 0.17567687\n",
      "Iteration 319, loss = 0.17372763\n",
      "Iteration 320, loss = 0.17583226\n",
      "Iteration 321, loss = 0.17992793\n",
      "Iteration 322, loss = 0.17224334\n",
      "Iteration 323, loss = 0.17312622\n",
      "Iteration 324, loss = 0.17166886\n",
      "Iteration 325, loss = 0.16969609\n",
      "Iteration 326, loss = 0.16798458\n",
      "Iteration 327, loss = 0.16697228\n",
      "Iteration 328, loss = 0.16726975\n",
      "Iteration 329, loss = 0.16644405\n",
      "Iteration 330, loss = 0.16945243\n",
      "Iteration 331, loss = 0.16342212\n",
      "Iteration 332, loss = 0.16314012\n",
      "Iteration 333, loss = 0.16506597\n",
      "Iteration 334, loss = 0.16311713\n",
      "Iteration 335, loss = 0.17031955\n",
      "Iteration 336, loss = 0.16217833\n",
      "Iteration 337, loss = 0.16094116\n",
      "Iteration 338, loss = 0.16019119\n",
      "Iteration 339, loss = 0.16246770\n",
      "Iteration 340, loss = 0.15791164\n",
      "Iteration 341, loss = 0.16008861\n",
      "Iteration 342, loss = 0.16180371\n",
      "Iteration 343, loss = 0.15934299\n",
      "Iteration 344, loss = 0.15605506\n",
      "Iteration 345, loss = 0.15362643\n",
      "Iteration 346, loss = 0.15200301\n",
      "Iteration 347, loss = 0.15216051\n",
      "Iteration 348, loss = 0.15189165\n",
      "Iteration 349, loss = 0.15320675\n",
      "Iteration 350, loss = 0.15246732\n",
      "Iteration 351, loss = 0.15472468\n",
      "Iteration 352, loss = 0.14866435\n",
      "Iteration 353, loss = 0.14864156\n",
      "Iteration 354, loss = 0.14851519\n",
      "Iteration 355, loss = 0.14954308\n",
      "Iteration 356, loss = 0.15035986\n",
      "Iteration 357, loss = 0.14875339\n",
      "Iteration 358, loss = 0.14522547\n",
      "Iteration 359, loss = 0.14444151\n",
      "Iteration 360, loss = 0.14449593\n",
      "Iteration 361, loss = 0.14330936\n",
      "Iteration 362, loss = 0.14719892\n",
      "Iteration 363, loss = 0.14362749\n",
      "Iteration 364, loss = 0.14511772\n",
      "Iteration 365, loss = 0.14065353\n",
      "Iteration 366, loss = 0.14241517\n",
      "Iteration 367, loss = 0.14338610\n",
      "Iteration 368, loss = 0.13848039\n",
      "Iteration 369, loss = 0.13694128\n",
      "Iteration 370, loss = 0.13858073\n",
      "Iteration 371, loss = 0.13648933\n",
      "Iteration 372, loss = 0.13546219\n",
      "Iteration 373, loss = 0.13478795\n",
      "Iteration 374, loss = 0.13507128\n",
      "Iteration 375, loss = 0.13411591\n",
      "Iteration 376, loss = 0.13297357\n",
      "Iteration 377, loss = 0.13400700\n",
      "Iteration 378, loss = 0.13338869\n",
      "Iteration 379, loss = 0.13289144\n",
      "Iteration 380, loss = 0.13275809\n",
      "Iteration 381, loss = 0.13264973\n",
      "Iteration 382, loss = 0.13085982\n",
      "Iteration 383, loss = 0.13092126\n",
      "Iteration 384, loss = 0.13080386\n",
      "Iteration 385, loss = 0.13285897\n",
      "Iteration 386, loss = 0.13108112\n",
      "Iteration 387, loss = 0.12882008\n",
      "Iteration 388, loss = 0.12725605\n",
      "Iteration 389, loss = 0.12824410\n",
      "Iteration 390, loss = 0.12659922\n",
      "Iteration 391, loss = 0.12624821\n",
      "Iteration 392, loss = 0.12577173\n",
      "Iteration 393, loss = 0.12445552\n",
      "Iteration 394, loss = 0.12386186\n",
      "Iteration 395, loss = 0.12349126\n",
      "Iteration 396, loss = 0.12283961\n",
      "Iteration 397, loss = 0.12202227\n",
      "Iteration 398, loss = 0.12172188\n",
      "Iteration 399, loss = 0.12184651\n",
      "Iteration 400, loss = 0.12034634\n",
      "Iteration 401, loss = 0.12137081\n",
      "Iteration 402, loss = 0.11994026\n",
      "Iteration 403, loss = 0.11981245\n",
      "Iteration 404, loss = 0.12100453\n",
      "Iteration 405, loss = 0.11965509\n",
      "Iteration 406, loss = 0.11976351\n",
      "Iteration 407, loss = 0.11691898\n",
      "Iteration 408, loss = 0.11775374\n",
      "Iteration 409, loss = 0.11740266\n",
      "Iteration 410, loss = 0.11750560\n",
      "Iteration 411, loss = 0.11819419\n",
      "Iteration 412, loss = 0.11925031\n",
      "Iteration 413, loss = 0.11566363\n",
      "Iteration 414, loss = 0.11659545\n",
      "Iteration 415, loss = 0.11719896\n",
      "Iteration 416, loss = 0.11523354\n",
      "Iteration 417, loss = 0.11265655\n",
      "Iteration 418, loss = 0.11371101\n",
      "Iteration 419, loss = 0.11309910\n",
      "Iteration 420, loss = 0.11262416\n",
      "Iteration 421, loss = 0.11332964\n",
      "Iteration 422, loss = 0.11096442\n",
      "Iteration 423, loss = 0.11159889\n",
      "Iteration 424, loss = 0.11205016\n",
      "Iteration 425, loss = 0.11071176\n",
      "Iteration 426, loss = 0.11070165\n",
      "Iteration 427, loss = 0.10791697\n",
      "Iteration 428, loss = 0.10793927\n",
      "Iteration 429, loss = 0.10703467\n",
      "Iteration 430, loss = 0.10803292\n",
      "Iteration 431, loss = 0.10604216\n",
      "Iteration 432, loss = 0.10621369\n",
      "Iteration 433, loss = 0.10781824\n",
      "Iteration 434, loss = 0.10772926\n",
      "Iteration 435, loss = 0.11502966\n",
      "Iteration 436, loss = 0.11684193\n",
      "Iteration 437, loss = 0.11188679\n",
      "Iteration 438, loss = 0.11534099\n",
      "Iteration 439, loss = 0.10649003\n",
      "Iteration 440, loss = 0.10504518\n",
      "Iteration 441, loss = 0.10275438\n",
      "Iteration 442, loss = 0.10366287\n",
      "Iteration 443, loss = 0.10339290\n",
      "Iteration 444, loss = 0.10207471\n",
      "Iteration 445, loss = 0.10195789\n",
      "Iteration 446, loss = 0.10011995\n",
      "Iteration 447, loss = 0.09985948\n",
      "Iteration 448, loss = 0.10159220\n",
      "Iteration 449, loss = 0.10076254\n",
      "Iteration 450, loss = 0.10419781\n",
      "Iteration 451, loss = 0.10247885\n",
      "Iteration 452, loss = 0.09940381\n",
      "Iteration 453, loss = 0.10084513\n",
      "Iteration 454, loss = 0.10064377\n",
      "Iteration 455, loss = 0.09799021\n",
      "Iteration 456, loss = 0.09651236\n",
      "Iteration 457, loss = 0.09730545\n",
      "Iteration 458, loss = 0.09793643\n",
      "Iteration 459, loss = 0.09576479\n",
      "Iteration 460, loss = 0.09560404\n",
      "Iteration 461, loss = 0.09638574\n",
      "Iteration 462, loss = 0.09488589\n",
      "Iteration 463, loss = 0.09504246\n",
      "Iteration 464, loss = 0.09337935\n",
      "Iteration 465, loss = 0.09367255\n",
      "Iteration 466, loss = 0.09285499\n",
      "Iteration 467, loss = 0.09362415\n",
      "Iteration 468, loss = 0.09455665\n",
      "Iteration 469, loss = 0.09725549\n",
      "Iteration 470, loss = 0.09405126\n",
      "Iteration 471, loss = 0.09121626\n",
      "Iteration 472, loss = 0.09545119\n",
      "Iteration 473, loss = 0.09197909\n",
      "Iteration 474, loss = 0.09615556\n",
      "Iteration 475, loss = 0.08975991\n",
      "Iteration 476, loss = 0.08999869\n",
      "Iteration 477, loss = 0.08857011\n",
      "Iteration 478, loss = 0.08852020\n",
      "Iteration 479, loss = 0.08776649\n",
      "Iteration 480, loss = 0.08793801\n",
      "Iteration 481, loss = 0.08739298\n",
      "Iteration 482, loss = 0.09057977\n",
      "Iteration 483, loss = 0.09056867\n",
      "Iteration 484, loss = 0.08967023\n",
      "Iteration 485, loss = 0.08879514\n",
      "Iteration 486, loss = 0.08807623\n",
      "Iteration 487, loss = 0.08704189\n",
      "Iteration 488, loss = 0.08988220\n",
      "Iteration 489, loss = 0.08874895\n",
      "Iteration 490, loss = 0.08734197\n",
      "Iteration 491, loss = 0.08567566\n",
      "Iteration 492, loss = 0.08495219\n",
      "Iteration 493, loss = 0.08318983\n",
      "Iteration 494, loss = 0.08278600\n",
      "Iteration 495, loss = 0.08257778\n",
      "Iteration 496, loss = 0.08437189\n",
      "Iteration 497, loss = 0.08312214\n",
      "Iteration 498, loss = 0.08661645\n",
      "Iteration 499, loss = 0.08782430\n",
      "Iteration 500, loss = 0.09500350\n",
      "Iteration 501, loss = 0.08615953\n",
      "Iteration 502, loss = 0.08237945\n",
      "Iteration 503, loss = 0.08223338\n",
      "Iteration 504, loss = 0.08187342\n",
      "Iteration 505, loss = 0.08028949\n",
      "Iteration 506, loss = 0.08159586\n",
      "Iteration 507, loss = 0.07921943\n",
      "Iteration 508, loss = 0.07852482\n",
      "Iteration 509, loss = 0.07800777\n",
      "Iteration 510, loss = 0.07724877\n",
      "Iteration 511, loss = 0.07717444\n",
      "Iteration 512, loss = 0.07727691\n",
      "Iteration 513, loss = 0.08099777\n",
      "Iteration 514, loss = 0.08173037\n",
      "Iteration 515, loss = 0.07970699\n",
      "Iteration 516, loss = 0.07705608\n",
      "Iteration 517, loss = 0.07673023\n",
      "Iteration 518, loss = 0.07463763\n",
      "Iteration 519, loss = 0.07468903\n",
      "Iteration 520, loss = 0.07397146\n",
      "Iteration 521, loss = 0.07389689\n",
      "Iteration 522, loss = 0.07400238\n",
      "Iteration 523, loss = 0.07552250\n",
      "Iteration 524, loss = 0.07319646\n",
      "Iteration 525, loss = 0.07294602\n",
      "Iteration 526, loss = 0.07206402\n",
      "Iteration 527, loss = 0.07213758\n",
      "Iteration 528, loss = 0.07211376\n",
      "Iteration 529, loss = 0.07171764\n",
      "Iteration 530, loss = 0.07695574\n",
      "Iteration 531, loss = 0.07470335\n",
      "Iteration 532, loss = 0.07357712\n",
      "Iteration 533, loss = 0.07066919\n",
      "Iteration 534, loss = 0.07162117\n",
      "Iteration 535, loss = 0.07014886\n",
      "Iteration 536, loss = 0.06987509\n",
      "Iteration 537, loss = 0.07005790\n",
      "Iteration 538, loss = 0.07168661\n",
      "Iteration 539, loss = 0.07236595\n",
      "Iteration 540, loss = 0.06820852\n",
      "Iteration 541, loss = 0.06847755\n",
      "Iteration 542, loss = 0.06790826\n",
      "Iteration 543, loss = 0.06799578\n",
      "Iteration 544, loss = 0.06700145\n",
      "Iteration 545, loss = 0.06732312\n",
      "Iteration 546, loss = 0.06853500\n",
      "Iteration 547, loss = 0.06772711\n",
      "Iteration 548, loss = 0.06711197\n",
      "Iteration 549, loss = 0.06573128\n",
      "Iteration 550, loss = 0.06511696\n",
      "Iteration 551, loss = 0.06659925\n",
      "Iteration 552, loss = 0.06609799\n",
      "Iteration 553, loss = 0.06539551\n",
      "Iteration 554, loss = 0.06498088\n",
      "Iteration 555, loss = 0.06560152\n",
      "Iteration 556, loss = 0.06503769\n",
      "Iteration 557, loss = 0.06349488\n",
      "Iteration 558, loss = 0.06383425\n",
      "Iteration 559, loss = 0.06339737\n",
      "Iteration 560, loss = 0.06482009\n",
      "Iteration 561, loss = 0.06620204\n",
      "Iteration 562, loss = 0.06441228\n",
      "Iteration 563, loss = 0.06340460\n",
      "Iteration 564, loss = 0.06333502\n",
      "Iteration 565, loss = 0.06181061\n",
      "Iteration 566, loss = 0.06320451\n",
      "Iteration 567, loss = 0.06225771\n",
      "Iteration 568, loss = 0.06121491\n",
      "Iteration 569, loss = 0.06144329\n",
      "Iteration 570, loss = 0.06072053\n",
      "Iteration 571, loss = 0.05980534\n",
      "Iteration 572, loss = 0.06003277\n",
      "Iteration 573, loss = 0.06017777\n",
      "Iteration 574, loss = 0.06003242\n",
      "Iteration 575, loss = 0.06078164\n",
      "Iteration 576, loss = 0.06053525\n",
      "Iteration 577, loss = 0.06085933\n",
      "Iteration 578, loss = 0.05930344\n",
      "Iteration 579, loss = 0.05952240\n",
      "Iteration 580, loss = 0.05845577\n",
      "Iteration 581, loss = 0.05869246\n",
      "Iteration 582, loss = 0.05824642\n",
      "Iteration 583, loss = 0.05788252\n",
      "Iteration 584, loss = 0.05777320\n",
      "Iteration 585, loss = 0.05776606\n",
      "Iteration 586, loss = 0.05723545\n",
      "Iteration 587, loss = 0.05755599\n",
      "Iteration 588, loss = 0.05943368\n",
      "Iteration 589, loss = 0.06000510\n",
      "Iteration 590, loss = 0.05809186\n",
      "Iteration 591, loss = 0.05836878\n",
      "Iteration 592, loss = 0.05821825\n",
      "Iteration 593, loss = 0.06030337\n",
      "Iteration 594, loss = 0.06113607\n",
      "Iteration 595, loss = 0.05800004\n",
      "Iteration 596, loss = 0.05676033\n",
      "Iteration 597, loss = 0.05580323\n",
      "Iteration 598, loss = 0.05505138\n",
      "Iteration 599, loss = 0.05503375\n",
      "Iteration 600, loss = 0.05507838\n",
      "Iteration 601, loss = 0.05400350\n",
      "Iteration 602, loss = 0.05366704\n",
      "Iteration 603, loss = 0.05259621\n",
      "Iteration 604, loss = 0.05287083\n",
      "Iteration 605, loss = 0.05305633\n",
      "Iteration 606, loss = 0.05351692\n",
      "Iteration 607, loss = 0.05254449\n",
      "Iteration 608, loss = 0.05216524\n",
      "Iteration 609, loss = 0.05106919\n",
      "Iteration 610, loss = 0.05121652\n",
      "Iteration 611, loss = 0.05090680\n",
      "Iteration 612, loss = 0.05073970\n",
      "Iteration 613, loss = 0.05181172\n",
      "Iteration 614, loss = 0.05014551\n",
      "Iteration 615, loss = 0.05032337\n",
      "Iteration 616, loss = 0.04997015\n",
      "Iteration 617, loss = 0.05030134\n",
      "Iteration 618, loss = 0.04979887\n",
      "Iteration 619, loss = 0.05127971\n",
      "Iteration 620, loss = 0.04928372\n",
      "Iteration 621, loss = 0.04940003\n",
      "Iteration 622, loss = 0.04926686\n",
      "Iteration 623, loss = 0.04859660\n",
      "Iteration 624, loss = 0.04851969\n",
      "Iteration 625, loss = 0.04782509\n",
      "Iteration 626, loss = 0.04932532\n",
      "Iteration 627, loss = 0.04732670\n",
      "Iteration 628, loss = 0.04708492\n",
      "Iteration 629, loss = 0.04775756\n",
      "Iteration 630, loss = 0.04757741\n",
      "Iteration 631, loss = 0.04651533\n",
      "Iteration 632, loss = 0.04809327\n",
      "Iteration 633, loss = 0.04601818\n",
      "Iteration 634, loss = 0.04564142\n",
      "Iteration 635, loss = 0.04533737\n",
      "Iteration 636, loss = 0.04620540\n",
      "Iteration 637, loss = 0.04554954\n",
      "Iteration 638, loss = 0.04501959\n",
      "Iteration 639, loss = 0.04451523\n",
      "Iteration 640, loss = 0.04408839\n",
      "Iteration 641, loss = 0.04408101\n",
      "Iteration 642, loss = 0.04417025\n",
      "Iteration 643, loss = 0.04528793\n",
      "Iteration 644, loss = 0.04390614\n",
      "Iteration 645, loss = 0.04378713\n",
      "Iteration 646, loss = 0.04339239\n",
      "Iteration 647, loss = 0.04276305\n",
      "Iteration 648, loss = 0.04278052\n",
      "Iteration 649, loss = 0.04268702\n",
      "Iteration 650, loss = 0.04298887\n",
      "Iteration 651, loss = 0.04225317\n",
      "Iteration 652, loss = 0.04209351\n",
      "Iteration 653, loss = 0.04174341\n",
      "Iteration 654, loss = 0.04153034\n",
      "Iteration 655, loss = 0.04124328\n",
      "Iteration 656, loss = 0.04149904\n",
      "Iteration 657, loss = 0.04110027\n",
      "Iteration 658, loss = 0.04123442\n",
      "Iteration 659, loss = 0.04111319\n",
      "Iteration 660, loss = 0.04120362\n",
      "Iteration 661, loss = 0.04126481\n",
      "Iteration 662, loss = 0.04077732\n",
      "Iteration 663, loss = 0.04067586\n",
      "Iteration 664, loss = 0.03985665\n",
      "Iteration 665, loss = 0.04013162\n",
      "Iteration 666, loss = 0.04002463\n",
      "Iteration 667, loss = 0.04024568\n",
      "Iteration 668, loss = 0.03935904\n",
      "Iteration 669, loss = 0.03950183\n",
      "Iteration 670, loss = 0.03924207\n",
      "Iteration 671, loss = 0.03919092\n",
      "Iteration 672, loss = 0.03909625\n",
      "Iteration 673, loss = 0.03921947\n",
      "Iteration 674, loss = 0.03912865\n",
      "Iteration 675, loss = 0.03915565\n",
      "Iteration 676, loss = 0.03878865\n",
      "Iteration 677, loss = 0.03836386\n",
      "Iteration 678, loss = 0.03848943\n",
      "Iteration 679, loss = 0.03801887\n",
      "Iteration 680, loss = 0.03814876\n",
      "Iteration 681, loss = 0.03851194\n",
      "Iteration 682, loss = 0.03885199\n",
      "Iteration 683, loss = 0.03904865\n",
      "Iteration 684, loss = 0.03808068\n",
      "Iteration 685, loss = 0.03765355\n",
      "Iteration 686, loss = 0.03750084\n",
      "Iteration 687, loss = 0.03603861\n",
      "Iteration 688, loss = 0.03554865\n",
      "Iteration 689, loss = 0.03616301\n",
      "Iteration 690, loss = 0.03619665\n",
      "Iteration 691, loss = 0.03634934\n",
      "Iteration 692, loss = 0.03538298\n",
      "Iteration 693, loss = 0.03495439\n",
      "Iteration 694, loss = 0.03407150\n",
      "Iteration 695, loss = 0.03399212\n",
      "Iteration 696, loss = 0.03370265\n",
      "Iteration 697, loss = 0.03333691\n",
      "Iteration 698, loss = 0.03312851\n",
      "Iteration 699, loss = 0.03269937\n",
      "Iteration 700, loss = 0.03313282\n",
      "Iteration 701, loss = 0.03474962\n",
      "Iteration 702, loss = 0.03325058\n",
      "Iteration 703, loss = 0.03336911\n",
      "Iteration 704, loss = 0.03274045\n",
      "Iteration 705, loss = 0.03190416\n",
      "Iteration 706, loss = 0.03177239\n",
      "Iteration 707, loss = 0.03140065\n",
      "Iteration 708, loss = 0.03136846\n",
      "Iteration 709, loss = 0.03160110\n",
      "Iteration 710, loss = 0.03113364\n",
      "Iteration 711, loss = 0.03164696\n",
      "Iteration 712, loss = 0.03146552\n",
      "Iteration 713, loss = 0.03139475\n",
      "Iteration 714, loss = 0.03084358\n",
      "Iteration 715, loss = 0.03041535\n",
      "Iteration 716, loss = 0.03051827\n",
      "Iteration 717, loss = 0.03024772\n",
      "Iteration 718, loss = 0.02983823\n",
      "Iteration 719, loss = 0.02973210\n",
      "Iteration 720, loss = 0.02961969\n",
      "Iteration 721, loss = 0.02934780\n",
      "Iteration 722, loss = 0.02941705\n",
      "Iteration 723, loss = 0.02941448\n",
      "Iteration 724, loss = 0.02935725\n",
      "Iteration 725, loss = 0.02896947\n",
      "Iteration 726, loss = 0.02889793\n",
      "Iteration 727, loss = 0.02921547\n",
      "Iteration 728, loss = 0.02960684\n",
      "Iteration 729, loss = 0.02934080\n",
      "Iteration 730, loss = 0.02831038\n",
      "Iteration 731, loss = 0.02811388\n",
      "Iteration 732, loss = 0.02799113\n",
      "Iteration 733, loss = 0.02831331\n",
      "Iteration 734, loss = 0.02798185\n",
      "Iteration 735, loss = 0.02757773\n",
      "Iteration 736, loss = 0.02767183\n",
      "Iteration 737, loss = 0.02734801\n",
      "Iteration 738, loss = 0.02730633\n",
      "Iteration 739, loss = 0.02725816\n",
      "Iteration 740, loss = 0.02815891\n",
      "Iteration 741, loss = 0.02724698\n",
      "Iteration 742, loss = 0.02711870\n",
      "Iteration 743, loss = 0.02820174\n",
      "Iteration 744, loss = 0.02709580\n",
      "Iteration 745, loss = 0.02716505\n",
      "Iteration 746, loss = 0.02615512\n",
      "Iteration 747, loss = 0.02621729\n",
      "Iteration 748, loss = 0.02602097\n",
      "Iteration 749, loss = 0.02583382\n",
      "Iteration 750, loss = 0.02608381\n",
      "Iteration 751, loss = 0.02552672\n",
      "Iteration 752, loss = 0.02595953\n",
      "Iteration 753, loss = 0.02554855\n",
      "Iteration 754, loss = 0.02536484\n",
      "Iteration 755, loss = 0.02541726\n",
      "Iteration 756, loss = 0.02546003\n",
      "Iteration 757, loss = 0.02518065\n",
      "Iteration 758, loss = 0.02484718\n",
      "Iteration 759, loss = 0.02458143\n",
      "Iteration 760, loss = 0.02476608\n",
      "Iteration 761, loss = 0.02475252\n",
      "Iteration 762, loss = 0.02537461\n",
      "Iteration 763, loss = 0.02463410\n",
      "Iteration 764, loss = 0.02444357\n",
      "Iteration 765, loss = 0.02454649\n",
      "Iteration 766, loss = 0.02490256\n",
      "Iteration 767, loss = 0.02507957\n",
      "Iteration 768, loss = 0.02447690\n",
      "Iteration 769, loss = 0.02394146\n",
      "Iteration 770, loss = 0.02375787\n",
      "Iteration 771, loss = 0.02337338\n",
      "Iteration 772, loss = 0.02362379\n",
      "Iteration 773, loss = 0.02321645\n",
      "Iteration 774, loss = 0.02340414\n",
      "Iteration 775, loss = 0.02316450\n",
      "Iteration 776, loss = 0.02302923\n",
      "Iteration 777, loss = 0.02306390\n",
      "Iteration 778, loss = 0.02284828\n",
      "Iteration 779, loss = 0.02266477\n",
      "Iteration 780, loss = 0.02264085\n",
      "Iteration 781, loss = 0.02283363\n",
      "Iteration 782, loss = 0.02318244\n",
      "Iteration 783, loss = 0.02295636\n",
      "Iteration 784, loss = 0.02243816\n",
      "Iteration 785, loss = 0.02225299\n",
      "Iteration 786, loss = 0.02189752\n",
      "Iteration 787, loss = 0.02226825\n",
      "Iteration 788, loss = 0.02175162\n",
      "Iteration 789, loss = 0.02157448\n",
      "Iteration 790, loss = 0.02171743\n",
      "Iteration 791, loss = 0.02159210\n",
      "Iteration 792, loss = 0.02150607\n",
      "Iteration 793, loss = 0.02191254\n",
      "Iteration 794, loss = 0.02215073\n",
      "Iteration 795, loss = 0.02198681\n",
      "Iteration 796, loss = 0.02165856\n",
      "Iteration 797, loss = 0.02189069\n",
      "Iteration 798, loss = 0.02130833\n",
      "Iteration 799, loss = 0.02168467\n",
      "Iteration 800, loss = 0.02103725\n",
      "Iteration 801, loss = 0.02119738\n",
      "Iteration 802, loss = 0.02203220\n",
      "Iteration 803, loss = 0.02123753\n",
      "Iteration 804, loss = 0.02064955\n",
      "Iteration 805, loss = 0.02066961\n",
      "Iteration 806, loss = 0.02039802\n",
      "Iteration 807, loss = 0.02029019\n",
      "Iteration 808, loss = 0.02020403\n",
      "Iteration 809, loss = 0.02021900\n",
      "Iteration 810, loss = 0.02023034\n",
      "Iteration 811, loss = 0.02008487\n",
      "Iteration 812, loss = 0.01991180\n",
      "Iteration 813, loss = 0.01995076\n",
      "Iteration 814, loss = 0.01971740\n",
      "Iteration 815, loss = 0.01960423\n",
      "Iteration 816, loss = 0.01975446\n",
      "Iteration 817, loss = 0.01947365\n",
      "Iteration 818, loss = 0.01939082\n",
      "Iteration 819, loss = 0.01947635\n",
      "Iteration 820, loss = 0.01946068\n",
      "Iteration 821, loss = 0.01940928\n",
      "Iteration 822, loss = 0.01928705\n",
      "Iteration 823, loss = 0.01960114\n",
      "Iteration 824, loss = 0.01971346\n",
      "Iteration 825, loss = 0.01951841\n",
      "Iteration 826, loss = 0.01920178\n",
      "Iteration 827, loss = 0.01909724\n",
      "Iteration 828, loss = 0.01952690\n",
      "Iteration 829, loss = 0.01966573\n",
      "Iteration 830, loss = 0.02006751\n",
      "Iteration 831, loss = 0.01911171\n",
      "Iteration 832, loss = 0.01862876\n",
      "Iteration 833, loss = 0.01883482\n",
      "Iteration 834, loss = 0.01851578\n",
      "Iteration 835, loss = 0.01845413\n",
      "Iteration 836, loss = 0.01838519\n",
      "Iteration 837, loss = 0.01828920\n",
      "Iteration 838, loss = 0.01799546\n",
      "Iteration 839, loss = 0.01826933\n",
      "Iteration 840, loss = 0.01821328\n",
      "Iteration 841, loss = 0.01800731\n",
      "Iteration 842, loss = 0.01779520\n",
      "Iteration 843, loss = 0.01774986\n",
      "Iteration 844, loss = 0.01775152\n",
      "Iteration 845, loss = 0.01795814\n",
      "Iteration 846, loss = 0.01837256\n",
      "Iteration 847, loss = 0.01844799\n",
      "Iteration 848, loss = 0.01864707\n",
      "Iteration 849, loss = 0.01843751\n",
      "Iteration 850, loss = 0.01772386\n",
      "Iteration 851, loss = 0.01736648\n",
      "Iteration 852, loss = 0.01714143\n",
      "Iteration 853, loss = 0.01692304\n",
      "Iteration 854, loss = 0.01689800\n",
      "Iteration 855, loss = 0.01692894\n",
      "Iteration 856, loss = 0.01682138\n",
      "Iteration 857, loss = 0.01678013\n",
      "Iteration 858, loss = 0.01688153\n",
      "Iteration 859, loss = 0.01652118\n",
      "Iteration 860, loss = 0.01647504\n",
      "Iteration 861, loss = 0.01631938\n",
      "Iteration 862, loss = 0.01640447\n",
      "Iteration 863, loss = 0.01649103\n",
      "Iteration 864, loss = 0.01643605\n",
      "Iteration 865, loss = 0.01653445\n",
      "Iteration 866, loss = 0.01648466\n",
      "Iteration 867, loss = 0.01633059\n",
      "Iteration 868, loss = 0.01674471\n",
      "Iteration 869, loss = 0.01605706\n",
      "Iteration 870, loss = 0.01606957\n",
      "Iteration 871, loss = 0.01612922\n",
      "Iteration 872, loss = 0.01581116\n",
      "Iteration 873, loss = 0.01600398\n",
      "Iteration 874, loss = 0.01573913\n",
      "Iteration 875, loss = 0.01558185\n",
      "Iteration 876, loss = 0.01544204\n",
      "Iteration 877, loss = 0.01537740\n",
      "Iteration 878, loss = 0.01541263\n",
      "Iteration 879, loss = 0.01550639\n",
      "Iteration 880, loss = 0.01533782\n",
      "Iteration 881, loss = 0.01534851\n",
      "Iteration 882, loss = 0.01522163\n",
      "Iteration 883, loss = 0.01520410\n",
      "Iteration 884, loss = 0.01537023\n",
      "Iteration 885, loss = 0.01510514\n",
      "Iteration 886, loss = 0.01500271\n",
      "Iteration 887, loss = 0.01498840\n",
      "Iteration 888, loss = 0.01490962\n",
      "Iteration 889, loss = 0.01483747\n",
      "Iteration 890, loss = 0.01485008\n",
      "Iteration 891, loss = 0.01495033\n",
      "Iteration 892, loss = 0.01495745\n",
      "Iteration 893, loss = 0.01499123\n",
      "Iteration 894, loss = 0.01475530\n",
      "Iteration 895, loss = 0.01459823\n",
      "Iteration 896, loss = 0.01455171\n",
      "Iteration 897, loss = 0.01448699\n",
      "Iteration 898, loss = 0.01466427\n",
      "Iteration 899, loss = 0.01466186\n",
      "Iteration 900, loss = 0.01484823\n",
      "Iteration 901, loss = 0.01455342\n",
      "Iteration 902, loss = 0.01426507\n",
      "Iteration 903, loss = 0.01415327\n",
      "Iteration 904, loss = 0.01408761\n",
      "Iteration 905, loss = 0.01411001\n",
      "Iteration 906, loss = 0.01402777\n",
      "Iteration 907, loss = 0.01404890\n",
      "Iteration 908, loss = 0.01393590\n",
      "Iteration 909, loss = 0.01401573\n",
      "Iteration 910, loss = 0.01410082\n",
      "Iteration 911, loss = 0.01388015\n",
      "Iteration 912, loss = 0.01400687\n",
      "Iteration 913, loss = 0.01399684\n",
      "Iteration 914, loss = 0.01371963\n",
      "Iteration 915, loss = 0.01382186\n",
      "Iteration 916, loss = 0.01361439\n",
      "Iteration 917, loss = 0.01372371\n",
      "Iteration 918, loss = 0.01383692\n",
      "Iteration 919, loss = 0.01412466\n",
      "Iteration 920, loss = 0.01370315\n",
      "Iteration 921, loss = 0.01400424\n",
      "Iteration 922, loss = 0.01358087\n",
      "Iteration 923, loss = 0.01365959\n",
      "Iteration 924, loss = 0.01361550\n",
      "Iteration 925, loss = 0.01344223\n",
      "Iteration 926, loss = 0.01349499\n",
      "Iteration 927, loss = 0.01348548\n",
      "Iteration 928, loss = 0.01359559\n",
      "Iteration 929, loss = 0.01325066\n",
      "Iteration 930, loss = 0.01329778\n",
      "Iteration 931, loss = 0.01308847\n",
      "Iteration 932, loss = 0.01306499\n",
      "Iteration 933, loss = 0.01312736\n",
      "Iteration 934, loss = 0.01292303\n",
      "Iteration 935, loss = 0.01283112\n",
      "Iteration 936, loss = 0.01296779\n",
      "Iteration 937, loss = 0.01341530\n",
      "Iteration 938, loss = 0.01302440\n",
      "Iteration 939, loss = 0.01381550\n",
      "Iteration 940, loss = 0.01400292\n",
      "Iteration 941, loss = 0.01295295\n",
      "Iteration 942, loss = 0.01298488\n",
      "Iteration 943, loss = 0.01270657\n",
      "Iteration 944, loss = 0.01256693\n",
      "Iteration 945, loss = 0.01269321\n",
      "Iteration 946, loss = 0.01243456\n",
      "Iteration 947, loss = 0.01233172\n",
      "Iteration 948, loss = 0.01245724\n",
      "Iteration 949, loss = 0.01238655\n",
      "Iteration 950, loss = 0.01257630\n",
      "Iteration 951, loss = 0.01243211\n",
      "Iteration 952, loss = 0.01232890\n",
      "Iteration 953, loss = 0.01233847\n",
      "Iteration 954, loss = 0.01235275\n",
      "Iteration 955, loss = 0.01236030\n",
      "Iteration 956, loss = 0.01227013\n",
      "Iteration 957, loss = 0.01233148\n",
      "Iteration 958, loss = 0.01241937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5, 5), max_iter=2000, verbose=True)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.fit(train_images_vectors,train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973333333333333"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.score(train_images_vectors,train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.score(test_images_vectors,test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = train_images_vectors.shape[1]\n",
    "NUM_CLASSES = len(image_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'F', 'A', ..., 'F', 'H', 'I'], dtype='<U1')"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = [ord(c) - ord(\"A\") for c in train_classes]\n",
    "test_classes = [ord(c) - ord(\"A\") for c in test_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "keras_nn = Sequential([\n",
    "    Input(INPUT_DIM),\n",
    "    Dense(12,activation= \"relu\", kernel_regularizer=L2(0.0001) ),\n",
    "    Dense(NUM_CLASSES,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                9420      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 9,550\n",
      "Trainable params: 9,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_nn.compile(loss = \"sparse_categorical_crossentropy\" , optimizer=\"adam\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "43/43 [==============================] - 1s 7ms/step - loss: 1.8030 - accuracy: 0.3667 - val_loss: 1.4468 - val_accuracy: 0.4867\n",
      "Epoch 2/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 1.2257 - accuracy: 0.6356 - val_loss: 1.0595 - val_accuracy: 0.7133\n",
      "Epoch 3/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.9165 - accuracy: 0.7704 - val_loss: 0.9001 - val_accuracy: 0.7800\n",
      "Epoch 4/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.7347 - accuracy: 0.8304 - val_loss: 0.7856 - val_accuracy: 0.7933\n",
      "Epoch 5/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.6210 - accuracy: 0.8430 - val_loss: 0.6754 - val_accuracy: 0.7933\n",
      "Epoch 6/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.8763 - val_loss: 0.6649 - val_accuracy: 0.8000\n",
      "Epoch 7/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.4988 - accuracy: 0.8785 - val_loss: 0.6082 - val_accuracy: 0.8267\n",
      "Epoch 8/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.8889 - val_loss: 0.5807 - val_accuracy: 0.8267\n",
      "Epoch 9/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.8926 - val_loss: 0.5991 - val_accuracy: 0.8000\n",
      "Epoch 10/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.9007 - val_loss: 0.5623 - val_accuracy: 0.8400\n",
      "Epoch 11/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.9030 - val_loss: 0.5523 - val_accuracy: 0.8400\n",
      "Epoch 12/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.9089 - val_loss: 0.5560 - val_accuracy: 0.8400\n",
      "Epoch 13/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.3642 - accuracy: 0.9089 - val_loss: 0.5368 - val_accuracy: 0.8533\n",
      "Epoch 14/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.9141 - val_loss: 0.5220 - val_accuracy: 0.8533\n",
      "Epoch 15/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.9222 - val_loss: 0.5303 - val_accuracy: 0.8533\n",
      "Epoch 16/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.9237 - val_loss: 0.5417 - val_accuracy: 0.8467\n",
      "Epoch 17/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.9252 - val_loss: 0.5251 - val_accuracy: 0.8600\n",
      "Epoch 18/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.9356 - val_loss: 0.5134 - val_accuracy: 0.8467\n",
      "Epoch 19/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2834 - accuracy: 0.9363 - val_loss: 0.5468 - val_accuracy: 0.8533\n",
      "Epoch 20/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.9444 - val_loss: 0.5295 - val_accuracy: 0.8733\n",
      "Epoch 21/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.9385 - val_loss: 0.5580 - val_accuracy: 0.8533\n",
      "Epoch 22/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.9400 - val_loss: 0.5350 - val_accuracy: 0.8600\n",
      "Epoch 23/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.9481 - val_loss: 0.5204 - val_accuracy: 0.8667\n",
      "Epoch 24/500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9489 - val_loss: 0.5911 - val_accuracy: 0.8400\n",
      "Epoch 25/500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.9400 - val_loss: 0.5832 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff0b7d69a0>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nn.fit(train_images_vectors, np.array(train_classes),\n",
    "    epochs = 500,\n",
    "    validation_split=0.1,\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor =  \"val_accuracy\", min_delta = 0.001, patience = 5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.9433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26005566120147705, 0.9433333277702332]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nn.evaluate(train_images_vectors,np.array(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4723 - accuracy: 0.8840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4723193943500519, 0.8840000033378601]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nn.evaluate(test_images_vectors,np.array(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('baseREC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f22bf35da797130dc9772e9bbbc1553556016bd9c002cdfb8138dc94e3b793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
